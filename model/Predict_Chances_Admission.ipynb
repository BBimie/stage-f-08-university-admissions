{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL: NEURAL NETWORK\n",
    "Used ANN as model with 3 hidden layers and Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0           1        337          118                  4  4.5   4.5  9.65   \n",
       "1           2        324          107                  4  4.0   4.5  8.87   \n",
       "2           3        316          104                  3  3.0   3.5  8.00   \n",
       "3           4        322          110                  3  3.5   2.5  8.67   \n",
       "4           5        314          103                  2  2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading dataset\n",
    "df_raw = pd.read_csv(\"../01-University-Admissions/data/Admission_Predict_Ver1.1.csv\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the S.No. column as its not required\n",
    "df_raw = df_raw.drop(\"Serial No.\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.00000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>316.472000</td>\n",
       "      <td>107.192000</td>\n",
       "      <td>3.114000</td>\n",
       "      <td>3.374000</td>\n",
       "      <td>3.48400</td>\n",
       "      <td>8.576440</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.72174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.295148</td>\n",
       "      <td>6.081868</td>\n",
       "      <td>1.143512</td>\n",
       "      <td>0.991004</td>\n",
       "      <td>0.92545</td>\n",
       "      <td>0.604813</td>\n",
       "      <td>0.496884</td>\n",
       "      <td>0.14114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>290.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.34000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>308.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>8.127500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>317.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.50000</td>\n",
       "      <td>8.560000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.72000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>325.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>9.040000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.82000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>340.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>9.920000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.97000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        GRE Score  TOEFL Score  University Rating         SOP       LOR   \\\n",
       "count  500.000000   500.000000         500.000000  500.000000  500.00000   \n",
       "mean   316.472000   107.192000           3.114000    3.374000    3.48400   \n",
       "std     11.295148     6.081868           1.143512    0.991004    0.92545   \n",
       "min    290.000000    92.000000           1.000000    1.000000    1.00000   \n",
       "25%    308.000000   103.000000           2.000000    2.500000    3.00000   \n",
       "50%    317.000000   107.000000           3.000000    3.500000    3.50000   \n",
       "75%    325.000000   112.000000           4.000000    4.000000    4.00000   \n",
       "max    340.000000   120.000000           5.000000    5.000000    5.00000   \n",
       "\n",
       "             CGPA    Research  Chance of Admit   \n",
       "count  500.000000  500.000000         500.00000  \n",
       "mean     8.576440    0.560000           0.72174  \n",
       "std      0.604813    0.496884           0.14114  \n",
       "min      6.800000    0.000000           0.34000  \n",
       "25%      8.127500    0.000000           0.63000  \n",
       "50%      8.560000    1.000000           0.72000  \n",
       "75%      9.040000    1.000000           0.82000  \n",
       "max      9.920000    1.000000           0.97000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GRE', 'TOEFL', 'Univ_rating', 'SOP', 'LOR', 'CGPA', 'Research',\n",
       "       'Chance_of_admit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replacing column names\n",
    "new_cols = ['GRE', 'TOEFL','Univ_rating','SOP','LOR','CGPA','Research','Chance_of_admit']\n",
    "df_raw.columns=new_cols\n",
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset\n",
    "X = df_raw[new_cols[:7]]\n",
    "Y = df_raw['Chance_of_admit']\n",
    "X_train = X[:400]\n",
    "X_test = X[400:]\n",
    "Y_train = Y[:400]\n",
    "Y_test = Y[400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE</th>\n",
       "      <th>TOEFL</th>\n",
       "      <th>Univ_rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>316.807500</td>\n",
       "      <td>107.410000</td>\n",
       "      <td>3.087500</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>3.452500</td>\n",
       "      <td>8.598925</td>\n",
       "      <td>0.547500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.473646</td>\n",
       "      <td>6.069514</td>\n",
       "      <td>1.143728</td>\n",
       "      <td>1.006869</td>\n",
       "      <td>0.898478</td>\n",
       "      <td>0.596317</td>\n",
       "      <td>0.498362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>290.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>308.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.170000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>317.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>8.610000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>325.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.062500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>340.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.920000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              GRE       TOEFL  Univ_rating         SOP         LOR  \\\n",
       "count  400.000000  400.000000   400.000000  400.000000  400.000000   \n",
       "mean   316.807500  107.410000     3.087500    3.400000    3.452500   \n",
       "std     11.473646    6.069514     1.143728    1.006869    0.898478   \n",
       "min    290.000000   92.000000     1.000000    1.000000    1.000000   \n",
       "25%    308.000000  103.000000     2.000000    2.500000    3.000000   \n",
       "50%    317.000000  107.000000     3.000000    3.500000    3.500000   \n",
       "75%    325.000000  112.000000     4.000000    4.000000    4.000000   \n",
       "max    340.000000  120.000000     5.000000    5.000000    5.000000   \n",
       "\n",
       "             CGPA    Research  \n",
       "count  400.000000  400.000000  \n",
       "mean     8.598925    0.547500  \n",
       "std      0.596317    0.498362  \n",
       "min      6.800000    0.000000  \n",
       "25%      8.170000    0.000000  \n",
       "50%      8.610000    1.000000  \n",
       "75%      9.062500    1.000000  \n",
       "max      9.920000    1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE</th>\n",
       "      <th>TOEFL</th>\n",
       "      <th>Univ_rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.536150</td>\n",
       "      <td>0.550357</td>\n",
       "      <td>0.521875</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.613125</td>\n",
       "      <td>0.576579</td>\n",
       "      <td>0.547500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.229473</td>\n",
       "      <td>0.216768</td>\n",
       "      <td>0.285932</td>\n",
       "      <td>0.251717</td>\n",
       "      <td>0.224619</td>\n",
       "      <td>0.191127</td>\n",
       "      <td>0.498362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.439103</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.580128</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.725160</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              GRE       TOEFL  Univ_rating         SOP         LOR  \\\n",
       "count  400.000000  400.000000   400.000000  400.000000  400.000000   \n",
       "mean     0.536150    0.550357     0.521875    0.600000    0.613125   \n",
       "std      0.229473    0.216768     0.285932    0.251717    0.224619   \n",
       "min      0.000000    0.000000     0.000000    0.000000    0.000000   \n",
       "25%      0.360000    0.392857     0.250000    0.375000    0.500000   \n",
       "50%      0.540000    0.535714     0.500000    0.625000    0.625000   \n",
       "75%      0.700000    0.714286     0.750000    0.750000    0.750000   \n",
       "max      1.000000    1.000000     1.000000    1.000000    1.000000   \n",
       "\n",
       "             CGPA    Research  \n",
       "count  400.000000  400.000000  \n",
       "mean     0.576579    0.547500  \n",
       "std      0.191127    0.498362  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.439103    0.000000  \n",
       "50%      0.580128    1.000000  \n",
       "75%      0.725160    1.000000  \n",
       "max      1.000000    1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scaling the dataset using MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "myScaler = MinMaxScaler()\n",
    "X_cols = list(X_train.columns)\n",
    "X_train = myScaler.fit_transform(X_train)\n",
    "X_test = myScaler.transform(X_test)\n",
    "X_train = pd.DataFrame(X_train, columns = X_cols)\n",
    "X_test = pd.DataFrame(X_test, columns = X_cols)\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE</th>\n",
       "      <th>TOEFL</th>\n",
       "      <th>Univ_rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance_of_admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GRE</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.827200</td>\n",
       "      <td>0.635376</td>\n",
       "      <td>0.613498</td>\n",
       "      <td>0.524679</td>\n",
       "      <td>0.825878</td>\n",
       "      <td>0.563398</td>\n",
       "      <td>0.810351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOEFL</th>\n",
       "      <td>0.827200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.649799</td>\n",
       "      <td>0.644410</td>\n",
       "      <td>0.541563</td>\n",
       "      <td>0.810574</td>\n",
       "      <td>0.467012</td>\n",
       "      <td>0.792228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Univ_rating</th>\n",
       "      <td>0.635376</td>\n",
       "      <td>0.649799</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.728024</td>\n",
       "      <td>0.608651</td>\n",
       "      <td>0.705254</td>\n",
       "      <td>0.427047</td>\n",
       "      <td>0.690132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOP</th>\n",
       "      <td>0.613498</td>\n",
       "      <td>0.644410</td>\n",
       "      <td>0.728024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663707</td>\n",
       "      <td>0.712154</td>\n",
       "      <td>0.408116</td>\n",
       "      <td>0.684137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOR</th>\n",
       "      <td>0.524679</td>\n",
       "      <td>0.541563</td>\n",
       "      <td>0.608651</td>\n",
       "      <td>0.663707</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.637469</td>\n",
       "      <td>0.372526</td>\n",
       "      <td>0.645365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CGPA</th>\n",
       "      <td>0.825878</td>\n",
       "      <td>0.810574</td>\n",
       "      <td>0.705254</td>\n",
       "      <td>0.712154</td>\n",
       "      <td>0.637469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.501311</td>\n",
       "      <td>0.882413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Research</th>\n",
       "      <td>0.563398</td>\n",
       "      <td>0.467012</td>\n",
       "      <td>0.427047</td>\n",
       "      <td>0.408116</td>\n",
       "      <td>0.372526</td>\n",
       "      <td>0.501311</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.545871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chance_of_admit</th>\n",
       "      <td>0.810351</td>\n",
       "      <td>0.792228</td>\n",
       "      <td>0.690132</td>\n",
       "      <td>0.684137</td>\n",
       "      <td>0.645365</td>\n",
       "      <td>0.882413</td>\n",
       "      <td>0.545871</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      GRE     TOEFL  Univ_rating       SOP       LOR  \\\n",
       "GRE              1.000000  0.827200     0.635376  0.613498  0.524679   \n",
       "TOEFL            0.827200  1.000000     0.649799  0.644410  0.541563   \n",
       "Univ_rating      0.635376  0.649799     1.000000  0.728024  0.608651   \n",
       "SOP              0.613498  0.644410     0.728024  1.000000  0.663707   \n",
       "LOR              0.524679  0.541563     0.608651  0.663707  1.000000   \n",
       "CGPA             0.825878  0.810574     0.705254  0.712154  0.637469   \n",
       "Research         0.563398  0.467012     0.427047  0.408116  0.372526   \n",
       "Chance_of_admit  0.810351  0.792228     0.690132  0.684137  0.645365   \n",
       "\n",
       "                     CGPA  Research  Chance_of_admit  \n",
       "GRE              0.825878  0.563398         0.810351  \n",
       "TOEFL            0.810574  0.467012         0.792228  \n",
       "Univ_rating      0.705254  0.427047         0.690132  \n",
       "SOP              0.712154  0.408116         0.684137  \n",
       "LOR              0.637469  0.372526         0.645365  \n",
       "CGPA             1.000000  0.501311         0.882413  \n",
       "Research         0.501311  1.000000         0.545871  \n",
       "Chance_of_admit  0.882413  0.545871         1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the Correlation\n",
    "df_raw.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Validation Split\n",
    "X_t = X_train[:300]\n",
    "X_val = X_train[300:400]\n",
    "Y_t = Y_train[:300]\n",
    "Y_val = Y_train[300:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELLING\n",
    "\n",
    "training_epochs = 50\n",
    "learning_rate = 0.021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 8)                 64        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 105\n",
      "Trainable params: 105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(4,activation='sigmoid'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate),metrics=[keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.0399 - root_mean_squared_error: 0.1997 - val_loss: 0.0198 - val_root_mean_squared_error: 0.1406\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0156 - root_mean_squared_error: 0.1251 - val_loss: 0.0109 - val_root_mean_squared_error: 0.1046\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0122 - root_mean_squared_error: 0.1105 - val_loss: 0.0082 - val_root_mean_squared_error: 0.0906\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0103 - root_mean_squared_error: 0.1014 - val_loss: 0.0068 - val_root_mean_squared_error: 0.0827\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0091 - root_mean_squared_error: 0.0952 - val_loss: 0.0052 - val_root_mean_squared_error: 0.0721\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0077 - root_mean_squared_error: 0.0876 - val_loss: 0.0050 - val_root_mean_squared_error: 0.0707\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0070 - root_mean_squared_error: 0.0839 - val_loss: 0.0047 - val_root_mean_squared_error: 0.0686\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0066 - root_mean_squared_error: 0.0815 - val_loss: 0.0046 - val_root_mean_squared_error: 0.0675\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0058 - root_mean_squared_error: 0.0761 - val_loss: 0.0042 - val_root_mean_squared_error: 0.0652\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0056 - root_mean_squared_error: 0.0749 - val_loss: 0.0046 - val_root_mean_squared_error: 0.0680\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0054 - root_mean_squared_error: 0.0733 - val_loss: 0.0042 - val_root_mean_squared_error: 0.0646\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0054 - root_mean_squared_error: 0.0733 - val_loss: 0.0046 - val_root_mean_squared_error: 0.0677\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0053 - root_mean_squared_error: 0.0725 - val_loss: 0.0041 - val_root_mean_squared_error: 0.0639\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0056 - root_mean_squared_error: 0.0750 - val_loss: 0.0040 - val_root_mean_squared_error: 0.0633\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0051 - root_mean_squared_error: 0.0717 - val_loss: 0.0045 - val_root_mean_squared_error: 0.0671\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0049 - root_mean_squared_error: 0.0700 - val_loss: 0.0044 - val_root_mean_squared_error: 0.0664\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0049 - root_mean_squared_error: 0.0698 - val_loss: 0.0041 - val_root_mean_squared_error: 0.0637\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0048 - root_mean_squared_error: 0.0689 - val_loss: 0.0041 - val_root_mean_squared_error: 0.0639\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0050 - root_mean_squared_error: 0.0704 - val_loss: 0.0039 - val_root_mean_squared_error: 0.0621\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0046 - root_mean_squared_error: 0.0682 - val_loss: 0.0036 - val_root_mean_squared_error: 0.0604\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0047 - root_mean_squared_error: 0.0688 - val_loss: 0.0040 - val_root_mean_squared_error: 0.0632\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0046 - root_mean_squared_error: 0.0676 - val_loss: 0.0036 - val_root_mean_squared_error: 0.0602\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0045 - root_mean_squared_error: 0.0674 - val_loss: 0.0041 - val_root_mean_squared_error: 0.0643\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0045 - root_mean_squared_error: 0.0669 - val_loss: 0.0043 - val_root_mean_squared_error: 0.0658\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0049 - root_mean_squared_error: 0.0697 - val_loss: 0.0037 - val_root_mean_squared_error: 0.0609\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0044 - root_mean_squared_error: 0.0665 - val_loss: 0.0037 - val_root_mean_squared_error: 0.0608\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0044 - root_mean_squared_error: 0.0665 - val_loss: 0.0035 - val_root_mean_squared_error: 0.0593\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0045 - root_mean_squared_error: 0.0669 - val_loss: 0.0042 - val_root_mean_squared_error: 0.0645\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0049 - root_mean_squared_error: 0.0698 - val_loss: 0.0038 - val_root_mean_squared_error: 0.0616\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.0045 - root_mean_squared_error: 0.0668 - val_loss: 0.0044 - val_root_mean_squared_error: 0.0662\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 0s 13ms/step - loss: 0.0043 - root_mean_squared_error: 0.0658 - val_loss: 0.0049 - val_root_mean_squared_error: 0.0701\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.0044 - root_mean_squared_error: 0.0660 - val_loss: 0.0035 - val_root_mean_squared_error: 0.0593\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0042 - root_mean_squared_error: 0.0649 - val_loss: 0.0035 - val_root_mean_squared_error: 0.0588\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0043 - root_mean_squared_error: 0.0659 - val_loss: 0.0036 - val_root_mean_squared_error: 0.0603\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0042 - root_mean_squared_error: 0.0648 - val_loss: 0.0035 - val_root_mean_squared_error: 0.0591\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0042 - root_mean_squared_error: 0.0649 - val_loss: 0.0033 - val_root_mean_squared_error: 0.0577\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0041 - root_mean_squared_error: 0.0643 - val_loss: 0.0035 - val_root_mean_squared_error: 0.0592\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0045 - root_mean_squared_error: 0.0672 - val_loss: 0.0036 - val_root_mean_squared_error: 0.0598\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0042 - root_mean_squared_error: 0.0652 - val_loss: 0.0043 - val_root_mean_squared_error: 0.0655\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0042 - root_mean_squared_error: 0.0649 - val_loss: 0.0037 - val_root_mean_squared_error: 0.0604\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0042 - root_mean_squared_error: 0.0645 - val_loss: 0.0036 - val_root_mean_squared_error: 0.0596\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0043 - root_mean_squared_error: 0.0652 - val_loss: 0.0040 - val_root_mean_squared_error: 0.0631\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0043 - root_mean_squared_error: 0.0657 - val_loss: 0.0037 - val_root_mean_squared_error: 0.0609\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0041 - root_mean_squared_error: 0.0637 - val_loss: 0.0036 - val_root_mean_squared_error: 0.0604\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0041 - root_mean_squared_error: 0.0640 - val_loss: 0.0034 - val_root_mean_squared_error: 0.0583\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0044 - root_mean_squared_error: 0.0666 - val_loss: 0.0035 - val_root_mean_squared_error: 0.0591\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0041 - root_mean_squared_error: 0.0640 - val_loss: 0.0042 - val_root_mean_squared_error: 0.0645\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 0s 6ms/step - loss: 0.0043 - root_mean_squared_error: 0.0659 - val_loss: 0.0033 - val_root_mean_squared_error: 0.0578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.0054 - root_mean_squared_error: 0.0732 - val_loss: 0.0034 - val_root_mean_squared_error: 0.0586\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 0.0044 - root_mean_squared_error: 0.0661 - val_loss: 0.0034 - val_root_mean_squared_error: 0.0583\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "\n",
    "results = model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs= training_epochs,\n",
    "    batch_size = 16,\n",
    "    validation_data=(X_val, Y_val.T)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting the training rmse scores vs epochs\n",
    "plt.plot(results.history['root_mean_squared_error'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for predictions is:  0.04681277174405588\n",
      "\n",
      "The Predicted values are:\n",
      "[0.6043093], [0.6657886], [0.7697755], [0.8709791], [0.5727227], [0.5067533], [0.6463817], [0.6273873], [0.5964915], [0.56626874], [0.55278325], [0.54426515], [0.5927707], [0.6014442], [0.7406797], [0.7797556], [0.60662985], [0.53412783], [0.67230314], [0.64990664], [0.5073082], [0.80039275], [0.81002855], [0.914811], [0.88994], [0.89987934], [0.7385413], [0.7494547], [0.7459992], [0.8970127], [0.6649963], [0.7686596], [0.84862], [0.74908876], [0.6174203], [0.60790336], [0.6128296], [0.6493206], [0.7473562], [0.68371665], [0.53084147], [0.7747351], [0.8888537], [0.8671403], [0.8734796], [0.88095593], [0.90111154], [0.7995831], [0.7634959], [0.76338536], [0.79427195], [0.8556961], [0.8945831], [0.7601243], [0.631336], [0.54487115], [0.5339251], [0.5004146], [0.6930295], [0.86885], [0.7598827], [0.5969185], [0.5944449], [0.58239084], [0.49240598], [0.6469523], [0.7460041], [0.77119243], [0.80261225], [0.85081387], [0.85553455], [0.6578683], [0.88238585], [0.6428445], [0.6131376], [0.53228647], [0.5782269], [0.5918355], [0.7392129], [0.8176956], [0.77499497], [0.7137127], [0.808766], [0.5570307], [0.6441147], [0.69030166], [0.65391546], [0.79755664], [0.77135605], [0.708468], [0.69982725], [0.5456205], [0.61300415], [0.5577071], [0.6270716], [0.85752094], [0.92513], [0.9133077], [0.7108137], [0.7922316], "
     ]
    }
   ],
   "source": [
    "# RMSE Score for Test Data (last 100 Entries of Data)\n",
    "\n",
    "y_pred = predictions\n",
    "mse = mean_squared_error(Y_test, y_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "print(\"RMSE for predictions is: \",rmse)\n",
    "print(\"\\nThe Predicted values are:\")\n",
    "for i in predictions:\n",
    "    print(i, end = ', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
